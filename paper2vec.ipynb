{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\my\\Miniconda3\\envs\\ml_python\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2017-11-14 00:06:31,299 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import timeline\n",
    "import networkx as nx\n",
    "from collections import defaultdict,namedtuple,Counter\n",
    "from glob import glob\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from six.moves import xrange\n",
    "if sys.version_info[0] >= 3:\n",
    "    unicode = str\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-14 00:06:31,770 done... 2708 papers loaded\n",
      "2017-11-14 00:06:31,770 7 labels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[CORA(words=['118', '125', '176', '252', '351', '456', '507', '521', '619', '648', '698', '702', '734', '845', '902', '1205', '1209', '1236', '1352', '1426'], tags=['31336']),\n",
       " CORA(words=['12', '509', '620', '763', '882', '893', '978', '1131', '1135', '1177', '1207', '1256', '1263', '1266', '1332', '1389', '1425'], tags=['1061127']),\n",
       " CORA(words=['45', '209', '212', '239', '292', '394', '510', '514', '581', '621', '623', '638', '1075', '1132', '1177', '1206', '1263', '1289', '1349', '1389', '1415', '1421'], tags=['1106406']),\n",
       " CORA(words=['41', '93', '99', '149', '594', '617', '624', '648', '874', '915', '942', '988', '1004', '1049', '1071', '1170', '1177', '1194', '1292', '1348', '1349'], tags=['13195']),\n",
       " CORA(words=['44', '122', '135', '153', '364', '396', '402', '474', '507', '619', '661', '699', '701', '828', '1066', '1174', '1175', '1177', '1208', '1209', '1212', '1254', '1381'], tags=['37879']),\n",
       " CORA(words=['93', '168', '211', '507', '526', '551', '874', '972', '1143', '1177', '1198', '1290', '1426'], tags=['1126012']),\n",
       " CORA(words=['187', '266', '317', '324', '367', '509', '619', '701', '916', '937', '1171', '1174', '1177', '1203', '1263', '1290', '1330', '1355', '1397'], tags=['1107140']),\n",
       " CORA(words=['3', '89', '99', '132', '203', '238', '438', '507', '521', '536', '638', '736', '807', '819', '921', '1046', '1177', '1192', '1198'], tags=['1102850'])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORA = namedtuple('CORA', 'words tags')\n",
    "\n",
    "datasets = []\n",
    "labels = defaultdict(list)\n",
    "with open(\"cora_content.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        ID = line[0]\n",
    "        labels[line[-1]].append(ID)\n",
    "        words = []\n",
    "        for i,w in enumerate(line[1:-1]):\n",
    "            if w == \"1\":\n",
    "                words.append(str(i))\n",
    "        datasets.append(\n",
    "            CORA(\n",
    "                words,\n",
    "                [ID]\n",
    "            )\n",
    "        )\n",
    "\n",
    "logging.info(\"done... %s papers loaded\" % (len(datasets)))\n",
    "logging.info(\"%s labels\" % (len(labels)))\n",
    "\n",
    "datasets[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-14 00:06:31,787 collecting all words and their counts\n",
      "2017-11-14 00:06:31,790 PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-11-14 00:06:31,810 collected 1432 word types and 2708 unique tags from a corpus of 2708 examples and 49216 words\n",
      "2017-11-14 00:06:31,811 Loading a fresh vocabulary\n",
      "2017-11-14 00:06:31,813 min_count=10 retains 968 unique words (67% of original 1432, drops 464)\n",
      "2017-11-14 00:06:31,817 min_count=10 leaves 46251 word corpus (93% of original 49216, drops 2965)\n",
      "2017-11-14 00:06:31,822 deleting the raw counts dictionary of 1432 items\n",
      "2017-11-14 00:06:31,824 sample=0.001 downsamples 76 most-common words\n",
      "2017-11-14 00:06:31,825 downsampling leaves estimated 39952 word corpus (86.4% of prior 46251)\n",
      "2017-11-14 00:06:31,827 estimated required memory for 968 words and 100 dimensions: 2883200 bytes\n",
      "2017-11-14 00:06:31,831 resetting layer weights\n",
      "2017-11-14 00:06:31,883 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:32,229 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:32,232 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:32,241 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:32,243 training on 246080 raw words (213189 effective words) took 0.4s, 604411 effective words/s\n",
      "2017-11-14 00:06:32,244 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:32,247 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:32,593 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:32,602 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:32,609 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:32,610 training on 246080 raw words (213540 effective words) took 0.4s, 602806 effective words/s\n",
      "2017-11-14 00:06:32,610 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:32,613 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:32,956 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:32,963 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:32,971 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:32,972 training on 246080 raw words (213408 effective words) took 0.4s, 604611 effective words/s\n",
      "2017-11-14 00:06:32,973 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:32,975 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:33,316 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:33,319 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:33,329 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:33,330 training on 246080 raw words (213359 effective words) took 0.3s, 610600 effective words/s\n",
      "2017-11-14 00:06:33,331 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:33,335 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:33,688 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:33,689 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:33,695 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:33,696 training on 246080 raw words (213221 effective words) took 0.4s, 602691 effective words/s\n",
      "2017-11-14 00:06:33,697 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:33,701 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:34,065 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:34,067 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:34,073 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:34,074 training on 246080 raw words (213274 effective words) took 0.4s, 584887 effective words/s\n",
      "2017-11-14 00:06:34,074 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:34,077 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:34,423 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:34,435 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:34,441 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:34,442 training on 246080 raw words (213501 effective words) took 0.4s, 602302 effective words/s\n",
      "2017-11-14 00:06:34,442 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:34,445 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:34,783 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:34,789 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:34,796 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:34,797 training on 246080 raw words (213432 effective words) took 0.3s, 621147 effective words/s\n",
      "2017-11-14 00:06:34,798 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:34,803 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:35,145 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:35,148 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:35,153 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:35,153 training on 246080 raw words (213125 effective words) took 0.3s, 619408 effective words/s\n",
      "2017-11-14 00:06:35,154 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:35,157 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:35,508 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:35,516 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:35,517 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:35,518 training on 246080 raw words (213176 effective words) took 0.4s, 601848 effective words/s\n",
      "2017-11-14 00:06:35,519 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:35,522 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:35,881 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:35,883 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:35,888 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:35,889 training on 246080 raw words (213483 effective words) took 0.4s, 593967 effective words/s\n",
      "2017-11-14 00:06:35,890 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:35,892 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:36,244 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:36,245 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:36,252 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:36,253 training on 246080 raw words (212959 effective words) took 0.4s, 599924 effective words/s\n",
      "2017-11-14 00:06:36,255 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-14 00:06:36,258 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:36,603 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:36,606 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:36,608 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:36,609 training on 246080 raw words (213239 effective words) took 0.3s, 620016 effective words/s\n",
      "2017-11-14 00:06:36,610 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:36,613 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:36,955 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:36,962 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:36,968 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:36,970 training on 246080 raw words (213431 effective words) took 0.4s, 605962 effective words/s\n",
      "2017-11-14 00:06:36,971 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:36,974 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:37,322 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:37,327 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:37,330 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:37,331 training on 246080 raw words (213061 effective words) took 0.4s, 608581 effective words/s\n",
      "2017-11-14 00:06:37,332 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:37,335 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:37,676 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:37,689 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:37,691 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:37,692 training on 246080 raw words (213370 effective words) took 0.4s, 608830 effective words/s\n",
      "2017-11-14 00:06:37,693 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:37,695 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:38,087 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:38,097 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:38,100 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:38,101 training on 246080 raw words (213279 effective words) took 0.4s, 533040 effective words/s\n",
      "2017-11-14 00:06:38,102 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:38,105 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:38,451 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:38,459 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:38,465 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:38,466 training on 246080 raw words (213362 effective words) took 0.4s, 601552 effective words/s\n",
      "2017-11-14 00:06:38,467 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:38,469 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:38,817 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:38,823 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:38,828 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:38,829 training on 246080 raw words (213124 effective words) took 0.4s, 605250 effective words/s\n",
      "2017-11-14 00:06:38,830 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:38,832 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:39,180 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:39,183 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:39,191 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:39,193 training on 246080 raw words (213246 effective words) took 0.4s, 601044 effective words/s\n",
      "2017-11-14 00:06:39,194 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:39,197 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:39,540 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:39,552 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:39,553 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:39,554 training on 246080 raw words (213511 effective words) took 0.3s, 611015 effective words/s\n",
      "2017-11-14 00:06:39,555 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:39,558 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:39,907 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:39,915 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:39,917 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:39,918 training on 246080 raw words (213334 effective words) took 0.4s, 602922 effective words/s\n",
      "2017-11-14 00:06:39,919 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:39,924 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:40,270 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:40,272 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:40,279 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:40,281 training on 246080 raw words (213182 effective words) took 0.4s, 608960 effective words/s\n",
      "2017-11-14 00:06:40,282 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:40,285 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:40,626 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:40,631 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:40,641 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:40,642 training on 246080 raw words (213333 effective words) took 0.4s, 608157 effective words/s\n",
      "2017-11-14 00:06:40,642 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:40,646 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:40,987 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:40,995 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:40,999 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:41,000 training on 246080 raw words (213401 effective words) took 0.3s, 613898 effective words/s\n",
      "2017-11-14 00:06:41,001 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:41,004 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:41,427 worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-14 00:06:41,433 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:41,435 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:41,436 training on 246080 raw words (213317 effective words) took 0.4s, 501783 effective words/s\n",
      "2017-11-14 00:06:41,437 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:41,440 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:41,827 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:41,834 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:41,835 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:41,836 training on 246080 raw words (213385 effective words) took 0.4s, 550770 effective words/s\n",
      "2017-11-14 00:06:41,837 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:41,841 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:42,188 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:42,209 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:42,211 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:42,213 training on 246080 raw words (213349 effective words) took 0.4s, 585364 effective words/s\n",
      "2017-11-14 00:06:42,213 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:42,216 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:42,599 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:42,602 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:42,608 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:42,610 training on 246080 raw words (213581 effective words) took 0.4s, 552423 effective words/s\n",
      "2017-11-14 00:06:42,610 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:42,613 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:42,957 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:42,963 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:42,966 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:42,966 training on 246080 raw words (213343 effective words) took 0.3s, 616251 effective words/s\n",
      "2017-11-14 00:06:42,967 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:42,969 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:43,311 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:43,325 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:43,330 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:43,331 training on 246080 raw words (213186 effective words) took 0.4s, 599038 effective words/s\n",
      "2017-11-14 00:06:43,332 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:43,335 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:43,720 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:43,722 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:43,725 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:43,726 training on 246080 raw words (213416 effective words) took 0.4s, 559358 effective words/s\n",
      "2017-11-14 00:06:43,727 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:43,730 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:44,077 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:44,081 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:44,088 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:44,089 training on 246080 raw words (213275 effective words) took 0.4s, 605672 effective words/s\n",
      "2017-11-14 00:06:44,090 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:44,094 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:44,448 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:44,453 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:44,458 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:44,460 training on 246080 raw words (213315 effective words) took 0.4s, 596966 effective words/s\n",
      "2017-11-14 00:06:44,462 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:44,465 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:44,822 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:44,830 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:44,834 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:44,835 training on 246080 raw words (213267 effective words) took 0.4s, 588760 effective words/s\n",
      "2017-11-14 00:06:44,836 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:44,839 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:45,197 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:45,199 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:45,201 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:45,201 training on 246080 raw words (213276 effective words) took 0.4s, 601704 effective words/s\n",
      "2017-11-14 00:06:45,202 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:45,205 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:45,543 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:45,556 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:45,563 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:45,564 training on 246080 raw words (213301 effective words) took 0.4s, 606222 effective words/s\n",
      "2017-11-14 00:06:45,565 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:45,568 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:45,913 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:45,922 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:45,928 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:45,929 training on 246080 raw words (213228 effective words) took 0.4s, 602488 effective words/s\n",
      "2017-11-14 00:06:45,931 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:45,934 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:46,285 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:46,288 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:46,297 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:46,298 training on 246080 raw words (213405 effective words) took 0.4s, 598781 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-14 00:06:46,299 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:46,302 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:46,648 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:46,651 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:46,662 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:46,665 training on 246080 raw words (213168 effective words) took 0.4s, 597106 effective words/s\n",
      "2017-11-14 00:06:46,666 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:46,670 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:47,022 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:47,028 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:47,032 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:47,033 training on 246080 raw words (213454 effective words) took 0.4s, 599765 effective words/s\n",
      "2017-11-14 00:06:47,034 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:47,037 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:47,395 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:47,397 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:47,402 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:47,403 training on 246080 raw words (213494 effective words) took 0.4s, 592933 effective words/s\n",
      "2017-11-14 00:06:47,405 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:47,408 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:47,759 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:47,763 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:47,772 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:47,773 training on 246080 raw words (213214 effective words) took 0.4s, 596583 effective words/s\n",
      "2017-11-14 00:06:47,774 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:47,777 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:48,129 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:48,139 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:48,145 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:48,145 training on 246080 raw words (213214 effective words) took 0.4s, 589996 effective words/s\n",
      "2017-11-14 00:06:48,146 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:48,151 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:48,559 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:48,569 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:48,572 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:48,573 training on 246080 raw words (213295 effective words) took 0.4s, 514652 effective words/s\n",
      "2017-11-14 00:06:48,574 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:48,576 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:48,931 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:48,933 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:48,935 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:48,935 training on 246080 raw words (213356 effective words) took 0.4s, 604220 effective words/s\n",
      "2017-11-14 00:06:48,936 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:48,939 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:49,312 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:49,314 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:49,321 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:49,322 training on 246080 raw words (213487 effective words) took 0.4s, 565224 effective words/s\n",
      "2017-11-14 00:06:49,323 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:49,326 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:49,694 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:49,698 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:49,705 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:49,705 training on 246080 raw words (213191 effective words) took 0.4s, 571343 effective words/s\n",
      "2017-11-14 00:06:49,706 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:49,709 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:50,037 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:50,052 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:50,055 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:50,056 training on 246080 raw words (213275 effective words) took 0.3s, 623633 effective words/s\n",
      "2017-11-14 00:06:50,057 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:50,060 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:50,399 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:50,404 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:50,411 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:50,412 training on 246080 raw words (213480 effective words) took 0.3s, 616442 effective words/s\n",
      "2017-11-14 00:06:50,412 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:50,415 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:50,756 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:50,762 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:50,766 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:50,767 training on 246080 raw words (213297 effective words) took 0.3s, 616162 effective words/s\n",
      "2017-11-14 00:06:50,767 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:50,770 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:51,115 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:51,117 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:51,119 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:51,120 training on 246080 raw words (213357 effective words) took 0.3s, 620345 effective words/s\n",
      "2017-11-14 00:06:51,121 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:51,123 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-14 00:06:51,458 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:51,466 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:51,469 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:51,470 training on 246080 raw words (213417 effective words) took 0.3s, 624092 effective words/s\n",
      "2017-11-14 00:06:51,471 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:51,474 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:51,811 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:51,820 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:51,824 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:51,825 training on 246080 raw words (213127 effective words) took 0.3s, 616575 effective words/s\n",
      "2017-11-14 00:06:51,826 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:51,829 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:52,158 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:52,168 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:52,173 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:52,173 training on 246080 raw words (213414 effective words) took 0.3s, 628878 effective words/s\n",
      "2017-11-14 00:06:52,174 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:52,177 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:52,510 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:52,515 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:52,524 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:52,525 training on 246080 raw words (213283 effective words) took 0.3s, 623099 effective words/s\n",
      "2017-11-14 00:06:52,526 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:52,528 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:52,863 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:52,867 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:52,875 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:52,876 training on 246080 raw words (213220 effective words) took 0.3s, 621869 effective words/s\n",
      "2017-11-14 00:06:52,877 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:52,879 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:53,220 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:53,221 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:53,225 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:53,226 training on 246080 raw words (213192 effective words) took 0.3s, 625394 effective words/s\n",
      "2017-11-14 00:06:53,227 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:53,229 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:53,566 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:53,569 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:53,576 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:53,577 training on 246080 raw words (213321 effective words) took 0.3s, 622411 effective words/s\n",
      "2017-11-14 00:06:53,578 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:53,580 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:53,911 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:53,915 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:53,923 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:53,924 training on 246080 raw words (213444 effective words) took 0.3s, 632309 effective words/s\n",
      "2017-11-14 00:06:53,925 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:53,927 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:54,262 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:54,267 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:54,276 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:54,277 training on 246080 raw words (213320 effective words) took 0.3s, 617877 effective words/s\n",
      "2017-11-14 00:06:54,278 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:54,281 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:54,617 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:54,618 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:54,625 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:54,625 training on 246080 raw words (213303 effective words) took 0.3s, 629758 effective words/s\n",
      "2017-11-14 00:06:54,626 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:54,630 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:54,978 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:54,980 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:54,984 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:54,985 training on 246080 raw words (213589 effective words) took 0.4s, 609445 effective words/s\n",
      "2017-11-14 00:06:54,986 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:54,989 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:55,331 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:55,337 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:55,339 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:55,340 training on 246080 raw words (213192 effective words) took 0.3s, 617010 effective words/s\n",
      "2017-11-14 00:06:55,340 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:55,343 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:55,683 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:55,690 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:55,696 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:55,697 training on 246080 raw words (213245 effective words) took 0.3s, 611125 effective words/s\n",
      "2017-11-14 00:06:55,698 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:55,701 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:56,051 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:56,056 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:56,061 worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-14 00:06:56,062 training on 246080 raw words (213367 effective words) took 0.4s, 600380 effective words/s\n",
      "2017-11-14 00:06:56,063 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:56,065 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:56,403 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:56,407 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:56,413 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:56,413 training on 246080 raw words (213312 effective words) took 0.3s, 622920 effective words/s\n",
      "2017-11-14 00:06:56,414 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:56,417 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:56,750 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:56,754 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:56,761 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:56,762 training on 246080 raw words (213314 effective words) took 0.3s, 628728 effective words/s\n",
      "2017-11-14 00:06:56,762 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:56,765 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:57,108 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:57,113 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:57,121 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:57,121 training on 246080 raw words (213561 effective words) took 0.4s, 609261 effective words/s\n",
      "2017-11-14 00:06:57,122 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:57,125 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:57,462 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:57,471 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:57,476 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:57,477 training on 246080 raw words (213288 effective words) took 0.3s, 616979 effective words/s\n",
      "2017-11-14 00:06:57,478 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:57,480 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:57,834 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:57,835 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:57,836 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:57,837 training on 246080 raw words (213512 effective words) took 0.4s, 608521 effective words/s\n",
      "2017-11-14 00:06:57,837 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:57,840 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:58,176 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:58,185 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:58,187 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:58,188 training on 246080 raw words (213138 effective words) took 0.3s, 619522 effective words/s\n",
      "2017-11-14 00:06:58,189 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:58,192 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:58,535 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:58,541 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:58,543 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:58,544 training on 246080 raw words (213157 effective words) took 0.3s, 615018 effective words/s\n",
      "2017-11-14 00:06:58,545 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:58,547 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:58,899 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:58,903 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:58,906 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:58,907 training on 246080 raw words (213130 effective words) took 0.4s, 601027 effective words/s\n",
      "2017-11-14 00:06:58,908 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:58,911 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:59,268 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:59,276 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:59,281 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:59,282 training on 246080 raw words (213268 effective words) took 0.4s, 585163 effective words/s\n",
      "2017-11-14 00:06:59,282 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:59,285 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:59,630 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:59,634 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:59,641 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:59,642 training on 246080 raw words (213287 effective words) took 0.4s, 607592 effective words/s\n",
      "2017-11-14 00:06:59,643 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:59,646 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:06:59,977 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:06:59,986 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:06:59,993 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:06:59,994 training on 246080 raw words (213307 effective words) took 0.3s, 621513 effective words/s\n",
      "2017-11-14 00:06:59,995 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:06:59,997 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:07:00,337 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:07:00,344 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:07:00,350 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:07:00,351 training on 246080 raw words (213105 effective words) took 0.3s, 613716 effective words/s\n",
      "2017-11-14 00:07:00,351 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:07:00,354 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:07:00,698 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:07:00,708 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:07:00,709 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:07:00,710 training on 246080 raw words (213477 effective words) took 0.3s, 610345 effective words/s\n",
      "2017-11-14 00:07:00,711 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-14 00:07:00,713 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:07:01,055 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:07:01,060 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:07:01,065 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:07:01,066 training on 246080 raw words (213311 effective words) took 0.3s, 615258 effective words/s\n",
      "2017-11-14 00:07:01,066 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:07:01,069 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:07:01,410 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:07:01,420 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:07:01,422 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:07:01,423 training on 246080 raw words (213343 effective words) took 0.3s, 613636 effective words/s\n",
      "2017-11-14 00:07:01,424 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:07:01,426 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:07:01,759 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:07:01,773 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:07:01,775 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:07:01,775 training on 246080 raw words (213295 effective words) took 0.3s, 620662 effective words/s\n",
      "2017-11-14 00:07:01,776 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:07:01,779 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:07:02,117 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:07:02,120 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:07:02,127 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:07:02,128 training on 246080 raw words (213532 effective words) took 0.3s, 622212 effective words/s\n",
      "2017-11-14 00:07:02,129 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:07:02,131 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:07:02,465 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:07:02,469 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:07:02,475 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:07:02,476 training on 246080 raw words (213135 effective words) took 0.3s, 628767 effective words/s\n",
      "2017-11-14 00:07:02,477 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:07:02,479 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:07:02,847 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:07:02,856 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:07:02,861 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:07:02,862 training on 246080 raw words (213240 effective words) took 0.4s, 566621 effective words/s\n",
      "2017-11-14 00:07:02,863 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:07:02,866 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:07:03,209 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:07:03,218 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:07:03,226 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:07:03,228 training on 246080 raw words (213133 effective words) took 0.4s, 597698 effective words/s\n",
      "2017-11-14 00:07:03,229 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:07:03,233 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:07:03,584 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:07:03,591 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:07:03,599 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:07:03,600 training on 246080 raw words (213267 effective words) took 0.4s, 597839 effective words/s\n",
      "2017-11-14 00:07:03,600 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:07:03,603 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:07:03,946 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:07:03,950 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:07:03,956 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:07:03,957 training on 246080 raw words (213378 effective words) took 0.3s, 612204 effective words/s\n",
      "2017-11-14 00:07:03,957 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:07:03,960 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:07:04,328 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:07:04,337 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:07:04,340 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:07:04,341 training on 246080 raw words (213430 effective words) took 0.4s, 568593 effective words/s\n",
      "2017-11-14 00:07:04,342 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:07:04,344 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:07:04,703 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:07:04,708 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:07:04,713 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:07:04,714 training on 246080 raw words (213325 effective words) took 0.4s, 587423 effective words/s\n",
      "2017-11-14 00:07:04,715 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:07:04,718 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:07:05,087 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:07:05,091 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:07:05,094 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:07:05,095 training on 246080 raw words (213304 effective words) took 0.4s, 573144 effective words/s\n",
      "2017-11-14 00:07:05,096 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:07:05,099 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:07:05,451 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:07:05,462 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:07:05,465 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:07:05,466 training on 246080 raw words (213444 effective words) took 0.4s, 591636 effective words/s\n",
      "2017-11-14 00:07:05,467 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:07:05,470 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:07:05,852 worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-14 00:07:05,854 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:07:05,861 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:07:05,862 training on 246080 raw words (213328 effective words) took 0.4s, 550936 effective words/s\n",
      "2017-11-14 00:07:05,863 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:07:05,866 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:07:06,205 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:07:06,213 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:07:06,218 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:07:06,218 training on 246080 raw words (213294 effective words) took 0.3s, 615720 effective words/s\n",
      "2017-11-14 00:07:06,219 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:07:06,222 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:07:06,562 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:07:06,564 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:07:06,573 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:07:06,574 training on 246080 raw words (213179 effective words) took 0.3s, 616698 effective words/s\n",
      "2017-11-14 00:07:06,574 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:07:06,577 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:07:06,912 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:07:06,913 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:07:06,922 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:07:06,923 training on 246080 raw words (213303 effective words) took 0.3s, 627114 effective words/s\n",
      "2017-11-14 00:07:06,923 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:07:06,926 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:07:07,267 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:07:07,268 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:07:07,275 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:07:07,276 training on 246080 raw words (213498 effective words) took 0.3s, 619319 effective words/s\n",
      "2017-11-14 00:07:07,277 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:07:07,280 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:07:07,604 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:07:07,621 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:07:07,624 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:07:07,625 training on 246080 raw words (213218 effective words) took 0.3s, 626650 effective words/s\n",
      "2017-11-14 00:07:07,626 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:07:07,628 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:07:07,964 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:07:07,968 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:07:07,976 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:07:07,977 training on 246080 raw words (213141 effective words) took 0.3s, 621749 effective words/s\n",
      "2017-11-14 00:07:07,977 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-14 00:07:07,980 training model with 3 workers on 968 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-14 00:07:08,315 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-14 00:07:08,324 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-14 00:07:08,329 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-14 00:07:08,330 training on 246080 raw words (213337 effective words) took 0.3s, 620012 effective words/s\n",
      "2017-11-14 00:07:08,331 under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(alpha=0.025, window=10, min_count=10, min_alpha=0.025, size=100)\n",
    "model.build_vocab(datasets)\n",
    "\n",
    "# decrease alpha\n",
    "for i in range(100):\n",
    "    random.shuffle(datasets)\n",
    "    model.alpha = 0.025-0.002*i\n",
    "    model.min_alpha = model.alpha\n",
    "    model.train(datasets,total_examples=model.corpus_count,epochs=model.iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.501477104874\n",
      "scores 0.562038404727\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "with open('doc2vec.embd','w') as f:\n",
    "    f.write(\"%s %s\\n\"%(len(datasets),100))\n",
    "    for y,key in enumerate(labels.keys()):\n",
    "        for index,paper in enumerate(labels[key]):\n",
    "            f.write(paper+\" \"+\" \".join([str(x) for x in model.docvecs[paper]])+\"\\n\")\n",
    "            X.append(model.docvecs[paper])\n",
    "            Y.append(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.5, random_state=0)\n",
    "clf = SVC(kernel='rbf',C=1.5).fit(X_train,y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "parameters = {\n",
    "    \"kernel\":[\"rbf\"],\n",
    "    \"C\" :[1.5]\n",
    "             }\n",
    "tunedclf = GridSearchCV(clf,parameters,cv=10,n_jobs=24)\n",
    "tunedclf.fit(X,Y)\n",
    "print(\"scores %s\" % tunedclf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27080\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "G = defaultdict(dict)\n",
    "\n",
    "for data in datasets:\n",
    "    for n in model.docvecs.most_similar(data.tags,topn=2):\n",
    "        G[data.tags[0]][n[0]] = None\n",
    "        G[n[0]][data.tags[0]] = None\n",
    "\n",
    "with open('cora_cites.txt') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip().split(\"\\t\")\n",
    "        try:\n",
    "            G[line[0]][line[1]] = None\n",
    "            G[line[1]][line[0]] = None\n",
    "        except:\n",
    "            print(line)\n",
    "            \n",
    "neighbors = []\n",
    "\n",
    "for i in range(10):\n",
    "    for node in G:\n",
    "        path = [node]\n",
    "        while len(path) < 60:\n",
    "            cur = path[-1]\n",
    "            path.append(random.choice(list(G[cur].keys())))\n",
    "        neighbors.append(path)\n",
    "        \n",
    "        \n",
    "print(len(neighbors))\n",
    "print(len(neighbors[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-12 12:03:08,204 collecting all words and their counts\n",
      "2017-11-12 12:03:08,206 PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-11-12 12:03:08,249 PROGRESS: at sentence #10000, processed 400000 words, keeping 2708 word types\n",
      "2017-11-12 12:03:08,298 PROGRESS: at sentence #20000, processed 800000 words, keeping 2708 word types\n",
      "2017-11-12 12:03:08,330 collected 2708 word types from a corpus of 1083200 raw words and 27080 sentences\n",
      "2017-11-12 12:03:08,331 Loading a fresh vocabulary\n",
      "2017-11-12 12:03:08,339 min_count=0 retains 2708 unique words (100% of original 2708, drops 0)\n",
      "2017-11-12 12:03:08,340 min_count=0 leaves 1083200 word corpus (100% of original 1083200, drops 0)\n",
      "2017-11-12 12:03:08,346 deleting the raw counts dictionary of 2708 items\n",
      "2017-11-12 12:03:08,347 sample=0.001 downsamples 4 most-common words\n",
      "2017-11-12 12:03:08,348 downsampling leaves estimated 1075207 word corpus (99.3% of prior 1083200)\n",
      "2017-11-12 12:03:08,348 estimated required memory for 2708 words and 100 dimensions: 3520400 bytes\n",
      "2017-11-12 12:03:08,355 resetting layer weights\n",
      "2017-11-12 12:03:08,381 loading projection weights from doc2vec.embd\n",
      "2017-11-12 12:03:08,565 merged 2708 vectors into (2708, 100) matrix from doc2vec.embd\n",
      "2017-11-12 12:03:08,566 training model with 3 workers on 2708 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-11-12 12:03:09,571 PROGRESS: at 41.73% examples, 2241202 words/s, in_qsize 5, out_qsize 0\n",
      "2017-11-12 12:03:10,575 PROGRESS: at 85.12% examples, 2282505 words/s, in_qsize 6, out_qsize 0\n",
      "2017-11-12 12:03:10,907 worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-12 12:03:10,911 worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-12 12:03:10,913 worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-12 12:03:10,913 training on 5416000 raw words (5376121 effective words) took 2.3s, 2294764 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5376121"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2v = Word2Vec(size=100, window=5, min_count=0)\n",
    "p2v.build_vocab(neighbors)\n",
    "p2v.intersect_word2vec_format('doc2vec.embd',lockf=1.0)\n",
    "p2v.train(neighbors,total_examples=p2v.corpus_count,epochs=p2v.iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.71758455 -0.66209936  0.55502558 -0.6359151   1.1250695  -1.21198142\n",
      "  0.23451109 -1.23037171 -0.1503356   0.73392546 -2.06334996 -0.36316052\n",
      "  0.49197263  1.66867948  1.02787614  1.40376604  0.77955592  0.98882478\n",
      "  0.47122076 -0.2451476   1.11152911  1.89864004  0.23590325  0.02930315\n",
      "  0.11828229 -1.43437624  1.1972779  -0.54909819  0.91523319  1.17575431\n",
      "  1.18605649 -0.88767278  1.67310548 -0.43302917  1.68476307 -0.60187632\n",
      " -0.6068747  -1.53292167  1.60054517  1.87925768 -1.18226838  1.35247755\n",
      " -1.19235623 -1.9220295   0.06113619 -0.78426415 -1.46965313 -0.82058495\n",
      "  0.43600863 -1.20224631 -0.1054199  -0.28657159 -0.2002437  -0.28128338\n",
      "  0.94277942 -0.12257486  0.30342183 -0.01108181  1.00861275  0.0807043\n",
      "  0.16269824  0.77411056  1.50581765  0.03296845  1.76792169  2.53235221\n",
      " -0.08404452 -1.31786275  0.62807137  0.2615113   1.11879659  0.64269465\n",
      "  0.50509471 -1.10541523  1.84277332 -2.20028782  0.09468788 -0.6896438\n",
      " -2.08605242 -2.37635541 -1.29334199  0.45332319  1.02921832  1.07530057\n",
      " -1.85822666 -0.24253078 -0.73230636 -0.77675486 -0.296931    1.2764101\n",
      "  1.35843706  0.91290945 -1.56025422  0.10288917 -1.11755645  1.17449665\n",
      "  0.55664539  1.61135864 -0.76771241  0.05344484]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for y,key in enumerate(labels.keys()):\n",
    "    for index,paper in enumerate(labels[key]):\n",
    "        X.append(p2v[paper])\n",
    "        Y.append(y)\n",
    "print(X[0])\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.801329394387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-12 12:03:24,732 scores 0.819423929099\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.5, random_state=42)\n",
    "clf = SVC(kernel='rbf',C=1.5).fit(X_train,y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "clf = SVC()\n",
    "parameters = {\n",
    "    \"kernel\":[\"rbf\"],\n",
    "    \"C\" :[1,10,100]\n",
    "             }\n",
    "tunedclf = GridSearchCV(clf,parameters,cv=10,n_jobs=24)\n",
    "tunedclf.fit(X,Y)\n",
    "logging.info(\"scores %s\" % tunedclf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
